{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-07T15:36:22.037314Z",
     "start_time": "2024-06-07T15:36:16.917400Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "iinfo(min=0, max=255, dtype=uint8)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.iinfo(torch.uint8)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-07T15:36:22.048833Z",
     "start_time": "2024-06-07T15:36:22.040289Z"
    }
   },
   "id": "eae2184abbfa4ef",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "iinfo(min=-128, max=127, dtype=int8)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.iinfo(torch.int8)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-07T15:36:22.059501Z",
     "start_time": "2024-06-07T15:36:22.050836Z"
    }
   },
   "id": "5c12f5fe1518f1ba",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "iinfo(min=0, max=65535, dtype=uint16)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.iinfo(torch.uint16)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-07T15:36:44.363130Z",
     "start_time": "2024-06-07T15:36:44.356488Z"
    }
   },
   "id": "36ada5651bf8acb9",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "iinfo(min=-32768, max=32767, dtype=int16)"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.iinfo(torch.int16)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-07T15:36:45.382085Z",
     "start_time": "2024-06-07T15:36:45.373852Z"
    }
   },
   "id": "499c3e55c06c1645",
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "In models you can use different data types. In above we're using a subset of signed and unsigned integers. You can clearly see how increasing the number of bytes increases the amount of values you can store, and how adding a sign bit affects the value range."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "50daae2044dcb64c"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "finfo(resolution=1e-06, min=-3.40282e+38, max=3.40282e+38, eps=1.19209e-07, smallest_normal=1.17549e-38, tiny=1.17549e-38, dtype=float32)"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.finfo(torch.float32)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-07T15:39:49.754168Z",
     "start_time": "2024-06-07T15:39:49.749188Z"
    }
   },
   "id": "225bdd2659d8842d",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "finfo(resolution=0.001, min=-65504, max=65504, eps=0.000976562, smallest_normal=6.10352e-05, tiny=6.10352e-05, dtype=float16)"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.finfo(torch.float16)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-07T15:39:51.716276Z",
     "start_time": "2024-06-07T15:39:51.710317Z"
    }
   },
   "id": "fa8c0f0e2d06727f",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "finfo(resolution=0.01, min=-3.38953e+38, max=3.38953e+38, eps=0.0078125, smallest_normal=1.17549e-38, tiny=1.17549e-38, dtype=bfloat16)"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.finfo(torch.bfloat16)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-07T15:39:52.453738Z",
     "start_time": "2024-06-07T15:39:52.449078Z"
    }
   },
   "id": "9ddfaf8e5ef10854",
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "source": [
    "For floats the story is largely similar to integers, except that they have an extra subset of bits dedicated to the fraction. Also, they don't tend to have unsigned versions.\n",
    "\n",
    "This shows in the 3 types above, where we have a larger value range, and the different in float types of the same amount of bytes determines whether we want a bigger range or precision."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5064c10cbaf0d4fc"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0.5353, 0.9768, 0.3072, 0.6751, 0.4463])"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_fp32 = torch.rand(100, dtype=torch.float32)\n",
    "tensor_fp32[:5]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-07T15:48:30.941829Z",
     "start_time": "2024-06-07T15:48:30.901823Z"
    }
   },
   "id": "59d9bfb62c6b2eb2",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0.5352, 0.9766, 0.3066, 0.6758, 0.4473], dtype=torch.bfloat16)"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_to_bf16 = tensor_fp32.to(dtype=torch.bfloat16)\n",
    "tensor_to_bf16[:5]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-07T15:48:52.673378Z",
     "start_time": "2024-06-07T15:48:52.663835Z"
    }
   },
   "id": "5400c3660b4a923",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0.5352, 0.9766, 0.3071, 0.6753, 0.4463], dtype=torch.float16)"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_to_fp16 = tensor_fp32.to(dtype=torch.float16)\n",
    "tensor_to_fp16[:5]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-07T15:49:17.638413Z",
     "start_time": "2024-06-07T15:49:17.632773Z"
    }
   },
   "id": "dd363ca107108c55",
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32 tensor: 0.333333343267440795898437500000000000000000000000000000000000\n",
      "BF16 tensor: 0.333984375000000000000000000000000000000000000000000000000000\n",
      "FP16 tensor: 0.333251953125000000000000000000000000000000000000000000000000\n"
     ]
    }
   ],
   "source": [
    "value_tensor = torch.tensor(1 / 3, dtype=torch.float32)\n",
    "value_tensor_bf16 = value_tensor.to(dtype=torch.bfloat16)\n",
    "value_tensor_fp16 = value_tensor.to(dtype=torch.float16)\n",
    "print(f\"FP32 tensor: {value_tensor.item():.60f}\")\n",
    "print(f\"BF16 tensor: {value_tensor_bf16.item():.60f}\")\n",
    "print(f\"FP16 tensor: {value_tensor_fp16.item():.60f}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-07T15:53:20.649481Z",
     "start_time": "2024-06-07T15:53:20.624457Z"
    }
   },
   "id": "4b644ee9ff6b5120",
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32 Byte count: 4\n",
      "BF16 Byte count: 2\n",
      "FP16 Byte count: 2\n"
     ]
    }
   ],
   "source": [
    "# check byte count of value tensor\n",
    "print(f\"FP32 Byte count: {value_tensor.numel() * value_tensor.element_size()}\")\n",
    "print(f\"BF16 Byte count: {value_tensor_bf16.numel() * value_tensor_bf16.element_size()}\")\n",
    "print(f\"FP16 Byte count: {value_tensor_fp16.numel() * value_tensor_fp16.element_size()}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-07T15:54:06.871054Z",
     "start_time": "2024-06-07T15:54:06.866361Z"
    }
   },
   "id": "190e05ef0e0479c4",
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the above example we apply downcasting, where we reduce a type to a lower precision. In the example used, you can see that the byte count is clearly halved. And while the precision of the values are reduced, it's not in a manner that is unacceptable."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8d94e05d70053895"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
